@ARTICLE{9204617,
  author={A. {Chatzitofis} and L. {Saroglou} and P. {Boutis} and P. {Drakoulis} and N. {Zioulis} and S. {Subramanyam} and B. {Kevelham} and C. {Charbonnier} and P. {Cesar} and D. {Zarpalas} and S. {Kollias} and P. {Daras}},
  journal={IEEE Access}, 
  title={HUMAN4D: A Human-Centric Multimodal Dataset for Motions and Immersive Media}, 
  year={2020},
  volume={8},
  number={},
  pages={176241-176262},
}

@article{doumanoglou2019benchmarking,
  title={Benchmarking Open-Source Static 3D Mesh Codecs for Immersive Media Interactive Live Streaming},
  author={Doumanoglou, Alexandros and Drakoulis, Petros and Zioulis, Nikolaos and Zarpalas, Dimitrios and Daras, Petros},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={9},
  number={1},
  pages={190--203},
  year={2019},
  publisher={IEEE},
  abstract={This work provides a systematic understanding of the requirements of live 3D mesh coding, targeting (tele-)immersive media streaming applications. We thoroughly benchmark in rate-distortion and runtime performance terms, four static 3D mesh coding solutions that are openly available. Apart from mesh geometry and connectivity, our analysis includes experiments for compressing vertex normals and attributes, something scarcely found in literature. Additionally, we provide a theoretical model of the tele-immersion pipeline that calculates its expected frame-rate, as well as lower and upper bounds for its end-to-end latency. In order to obtain these measures, the theoretical model takes into account the compression performance of the codecs and some indicative network conditions. Based on the results we obtained through our codec benchmarking, we used our theoretical model to calculate and provide concrete measures for these tele-immersion pipeline’s metrics and discuss on the optimal codec choice depending on the network setup. This offers deep insight into the available solutions and paves the way for future research},
  url_Pdf={https://www.iti.gr/iti/files/document/publications/JETCAS.2019.2898768.pdf},
  doi={10.1109/JETCAS.2019.2898768},
  url={https://doi.org/10.1109/JETCAS.2019.2898768},
}

@article{alvarez2019edge,
  title={An Edge-to-Cloud Virtualized Multimedia Service Platform for 5G Networks},
  author={Alvarez, Federico and Breitgand, David and Griffin, David and Andriani, Pasquale and Rizou, Stamatia and Zioulis, Nikolaos and Moscatelli, Francesca and Serrano, Javier and Keltsch, Madeleine and Trakadas, Panagiotis and others},
  journal={IEEE Transactions on Broadcasting},
  year={2019},
  publisher={IEEE},
  abstract={The focus of research into 5G networks to date has been largely on the required advances in network architectures, technologies and infrastructures. Less effort has been put on the applications and services that will make use of and exploit the flexibility of 5G networks built upon the concept of Software Defined Networking (SDN) and Network Function Virtualization (NFV). Media-based applications are amongst the most demanding services, requiring large bandwidths for high audio-visual quality, low-latency for interactivity and sufficient infrastructure resources to deliver the computational power for running the media applications in the networked cloud. This paper presents a novel Service Virtualization Platform (SVP), called 5G-MEDIA SVP, which leverages the principles of NFV and SDN to facilitate the development, deployment and operation of media services on 5G networks. The platform offers an advanced cognitive management environment for the provisioning of network services (NSs) and media-related applications, which directly link their lifecycle management with user experience as well as optimization of infrastructure resource utilization. Another innovation of 5G-MEDIA SVP is the integration of serverless computing with media intensive applications in 5G networks, increasing cost effectiveness of operation and simplifying development and deployment time. The proposed SVP is being validated against three media use cases: immersive Virtual Reality 3D gaming application, remote production of broadcast content incorporating user generated contents, and dynamically adaptive Content Distribution Networks (CDNs) for the intelligent distribution of Ultra-High Definition (UHD) content. The preliminary results of the 5GMEDIA SVP platform evaluation are compared against current practice and show that the proposed platform provides enhanced functionality for the operators and infrastructure owners, while ensuring better NS performance to service providers and end users. },
  url_Pdf={https://www.iti.gr/iti/files/document/publications/5G_Networks_final.pdf},
  doi={10.1109/TBC.2019.2901400},
  url={https://doi.org/10.1109/TBC.2019.2901400},
}

@article{doumanoglou2018quality,
  title={Quality of Experience for 3-D Immersive Media Streaming},
  author={Doumanoglou, Alexandros and Griffin, David and Serrano, Javier and Zioulis, Nikolaos and Phan, Truong Khoa and Jim{\'e}nez, David and Zarpalas, Dimitrios and Alvarez, Federico and Rio, Miguel and Daras, Petros},
  journal={IEEE Transactions on Broadcasting},
  volume={64},
  number={2},
  pages={379--391},
  year={2018},
  publisher={IEEE},
  doi={10.1109/TBC.2018.2823909},
   url={http://dx.doi.org/10.1109/TBC.2018.2823909},
  abstract={Recent advances in media capture and processing technologies have enabled new forms of true 3D media content that increase the degree of user immersion. The demand for more engaging forms of entertainment means that content distributors and broadcasters need to fine-tune their delivery mechanisms over the Internet as well as develop new models for quantifying and predicting user experience of these new forms of content. In the work described in this paper, we undertake one of the first studies into the QoE of real-time 3D media content streamed to VR headsets for entertainment purposes, in the context of game spectating. Our focus is on tele-immersive media that embed real users within virtual environments of interactive games. A key feature of engaging and realistic experiences in full 3D media environments, is allowing users unrestricted viewpoints. However, this comes at the cost of increased network bandwidth and the need of limiting network effects in order to transmit a realistic, real-time representation of the participants. The visual quality of 3D media is affected by geometry and texture parameters while the temporal aspects of smooth movement and synchronization are affected by lag introduced by network transmission effects. In this study, we investigate varying network conditions for a set of tele-immersive media sessions produced in a range of visual quality levels. Further, we investigate user navigation issues that inhibit free viewpoint VR spectating of live 3D media. After reporting on a study with multiple users we analyze the results and assess the overall QoE with respect to a range of visual quality and latency parameters. We propose a neural network QoE prediction model for 3D media, constructed from a combination of visual and network parameters.},
  url_Pdf={http://www.iti.gr/iti/files/document/publications/3D_Immersive_Media.pdf},
}

@article{alexiadis2018fast,
  title={Fast deformable model-based human performance capture and FVV using consumer-grade RGB-D sensors},
  author={Alexiadis, Dimitrios S and Zioulis, Nikolaos and Zarpalas, Dimitrios and Daras, Petros},
  journal={Pattern Recognition},
  volume={79},
  pages={260--278},
  year={2018},
  publisher={Elsevier},
  doi={10.1016/j.patcog.2018.02.013},
  url={http://dx.doi.org/10.1016/j.patcog.2018.02.013},
  abstract={In this paper, a novel end-to-end system for the fast reconstruction of human actor performances into 3D mesh sequences is proposed, using the input from a small set of consumer-grade RGB-Depth sensors. The proposed framework, by offline pre-reconstructing and employing a deformable actor’s 3D model to constrain the on-line reconstruction process, implicitly tracks the human motion. Handling non-rigid deformation of the 3D surface and applying appropriate texture mapping, it finally produces a dynamic sequence of temporally-coherent textured meshes, enabling realistic Free Viewpoint Video (FVV). Given the noisy input from a small set of low-cost sensors, the focus is on the fast (“quick-post”), robust and fully-automatic performance reconstruction. Apart from integrating existing ideas into a complete end-to-end system, which is itself a challenging task, several novel technical advances contribute to the speed, robustness and fidelity of the system, including a layered approach for model-based pose tracking, the definition and use of sophisticated energy functions, parallelizable on the GPU, as well as a new texture mapping scheme. The experimental results on a large number of challenging sequences, and comparisons with model-based and model-free approaches, demonstrate the efficiency of the proposed approach.},
  url_Pdf={http://www.iti.gr/iti/files/document/publications/RGB-D_09-03-2018.pdf},
  url_Supplementary={http://vcl.iti.gr/vclwordpress/wp-content/uploads/2017/06/Suppl_Main3.pdf},
  url_Website={http://vcl.iti.gr/performancecapture/},
  url_Data={http://vcl.iti.gr/dataset/dataset-of-multiple-kinect2-rgb-d-streams/},
}

@article{alexiadis2017integrated,
  title={An integrated platform for live 3d human reconstruction and motion capturing},
  author={Alexiadis, Dimitrios S and Chatzitofis, Anargyros and Zioulis, Nikolaos and Zoidi, Olga and Louizis, Georgios and Zarpalas, Dimitrios and Daras, Petros},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={27},
  number={4},
  pages={798--813},
  year={2017},
  publisher={IEEE},
  doi={10.1109/TCSVT.2016.2576922},
  url={http://dx.doi.org/10.1109/TCSVT.2016.2576922},
  abstract={The latest developments in 3D capturing, processing and rendering provide means to unlock novel 3D application pathways. The main elements of an integrated platform, which target Tele-Immersion (TI) and future 3D applications, are described in this paper, addressing the tasks of real-time capturing, robust 3D human shape/appearance reconstruction and skeletonbased motion tracking. More specifically, initially the details of a multiple RGB-D capturing system are given, along with a novel sensors’ calibration method. A robust, fast reconstruction method from multiple RGB-D streams is then proposed, based on an enhanced variation of the volumetric Fourier Transformbased method, parallelized on the GPU, accompanied with an appropriate texture mapping algorithm. On top of that, given the lack of relevant objective evaluation methods, a novel framework is proposed for the quantitative evaluation of realtime 3D reconstruction systems. Finally, a generic, multiple depth streams-based method for accurate real-time human skeleton tracking is proposed. Detailed experimental results with multi-Kinect2 datasets verify the validity of our arguments and the effectiveness of the proposed system and methodologies.},
  url_Pdf={http://www.iti.gr/iti/files/document/publications/An%20integrated%20platform%20for%20live%203D%20human.pdf},
  url_Supplementary={http://vcl.iti.gr/vclwordpress/wp-content/uploads/2016/02/Reconstruction_Supplementary_.pdf},
  url_Website={http://vcl.iti.gr/dataset/3d-reconstruction-and-skeleton-based-motion-tracking/},
  url_Data={http://vcl.iti.gr/dataset/datasets-of-multiple-kinect2-rgb-d-streams-and-skeleton-tracking/},
}